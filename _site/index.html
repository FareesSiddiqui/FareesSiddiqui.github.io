<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>An Introduction to Reinforcement Learning with Practical Examples – index</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2a7528f117d075273fca3a0b09f3bef2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-what-is-reinforcement-learning-rl" id="toc-introduction-what-is-reinforcement-learning-rl" class="nav-link active" data-scroll-target="#introduction-what-is-reinforcement-learning-rl">Introduction: What is Reinforcement Learning (RL)</a></li>
  <li><a href="#key-terminology-mathematical-background" id="toc-key-terminology-mathematical-background" class="nav-link" data-scroll-target="#key-terminology-mathematical-background">Key Terminology &amp; Mathematical Background</a>
  <ul>
  <li><a href="#policies" id="toc-policies" class="nav-link" data-scroll-target="#policies">Policies</a></li>
  <li><a href="#trajectories" id="toc-trajectories" class="nav-link" data-scroll-target="#trajectories">Trajectories</a></li>
  <li><a href="#rewards-and-return" id="toc-rewards-and-return" class="nav-link" data-scroll-target="#rewards-and-return">Rewards and Return</a></li>
  <li><a href="#the-rl-problem" id="toc-the-rl-problem" class="nav-link" data-scroll-target="#the-rl-problem">The RL Problem</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value Functions</a></li>
  <li><a href="#bellman-equations" id="toc-bellman-equations" class="nav-link" data-scroll-target="#bellman-equations">Bellman Equations</a>
  <ul class="collapse">
  <li><a href="#advantage-function" id="toc-advantage-function" class="nav-link" data-scroll-target="#advantage-function">Advantage Function</a></li>
  </ul></li>
  <li><a href="#policy-optimization" id="toc-policy-optimization" class="nav-link" data-scroll-target="#policy-optimization">Policy Optimization</a></li>
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q Learning</a></li>
  </ul></li>
  <li><a href="#solving-carracing-v3-using-proximal-policy-optimization-ppo" id="toc-solving-carracing-v3-using-proximal-policy-optimization-ppo" class="nav-link" data-scroll-target="#solving-carracing-v3-using-proximal-policy-optimization-ppo">Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">An Introduction to Reinforcement Learning with Practical Examples</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-what-is-reinforcement-learning-rl" class="level1">
<h1>Introduction: What is Reinforcement Learning (RL)</h1>
<p>I will start this article off with a little bit of mathematical background about Reinforcement Learning and how it works, so that readers have a strong base of knowledge and are able to understand some of the algorithms and methods at work.</p>
<p>Below is the layout of this article:</p>
<ul>
<li>What is Reinforcement Learning (RL)</li>
<li>Key Terminology
<ul>
<li>Policies</li>
<li>Trajectories</li>
<li>Rewards and Return</li>
<li>The formulation of an RL problem</li>
<li>Value Functions</li>
<li>Bellman Equations and the Advantage Function</li>
<li>Policy Optimization</li>
<li>Q Learning</li>
</ul></li>
<li>Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</li>
</ul>
<p>Reinforcement Learning (RL) is a machine learning paradigm inspired by how humans and animals learn through interactions with their environment. The core idea is that an <strong>agent</strong> learns to make decisions by interacting with an <strong>environment</strong>, aiming to maximize long term cumulative rewards.</p>
<p>The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees an observation of the state of the world (or environment), and then decides which action to take. The environment changes when the agent acts on it, but it may also change on its own.</p>
<p>The agent also perceives a <strong>reward</strong> from the environment. The reward is a number that tells the agent how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. This cumulative reward is known as the <strong>return</strong>. Reinforcement Learning methods are ways that the agent can learn behaviours to achieve its goal and maximize long term <em>return</em></p>
<p>Reinforcement Learning is built around the <strong>agent environment loop</strong>, as shown in the diagram below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images_quarto/agent_env.png" class="img-fluid figure-img"></p>
<figcaption>Agent Environment Paradigm</figcaption>
</figure>
</div>
</section>
<section id="key-terminology-mathematical-background" class="level1">
<h1>Key Terminology &amp; Mathematical Background</h1>
<p>Below I will explain how the agent environment interaction works, and the core terminology behind it as we will use it extensively later on in this article.</p>
<ul>
<li><p><strong>State <span class="math inline">\((S_t)\)</span></strong>: This is the current state of the environment. The environment provides the agent with a representation of its current situation, which we refer to as the state. For example, in a game, the state could be the positions of all characters on screen or in the scene.</p></li>
<li><p><strong>Action <span class="math inline">\((A_t)\)</span></strong>: Based on the current state, the agent chooses an action from its set of possible actions. The set of possible actions for an agent is known as the <strong>action space</strong>. In a simple self driving car, this could mean steering left, steering right, accelerate, or brake.</p></li>
</ul>
<section id="policies" class="level2">
<h2 class="anchored" data-anchor-id="policies">Policies</h2>
<p><strong>Policy <span class="math inline">\((\pi)\)</span></strong>: A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it’s usually denoted by <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
a_t = \mu(s_t)
\]</span></p>
<p>or it may be stochastic, in which case it is usually denoted by <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
a_t \sim \pi(\cdot | s_t)
\]</span></p>
<p><span class="math inline">\(a_t \sim \pi\)</span> means that <span class="math inline">\(a\)</span> is sampled from <span class="math inline">\(\pi\)</span>, since <span class="math inline">\(\pi\)</span> is a distribution.</p>
<p>In deep RL, we deal with <strong>parameterized policies</strong> which are policies whose outputs are computable functions that depend on a set of parameters (such as the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. Oftentimes the parameters of such a policy are denoted by <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\phi\)</span>, and are written as follow</p>
<p><span class="math display">\[
a_t = \mu_\theta(s_t) \ (deterministic \ policy)
\]</span> <span class="math display">\[
a_t \sim \pi_\theta(s_t) \ (stochastic \ policy)
\]</span></p>
</section>
<section id="trajectories" class="level2">
<h2 class="anchored" data-anchor-id="trajectories">Trajectories</h2>
<p>A trajectory <span class="math inline">\(\tau\)</span> is a sequence of states and actions in the world</p>
<p><span class="math display">\[
\tau = (s_0, a_0, s_1, a_1, \dots)
\]</span></p>
<p>The first state of the world, <span class="math inline">\(s_0\)</span> is randomly sampled from the <strong>start state distribution</strong>, which is sometimes denoted by <span class="math inline">\(\rho_0\)</span></p>
<p><span class="math display">\[
s_0 \sim \rho_0(\cdot)
\]</span></p>
<p>State transitions (what happens to the world between the state <span class="math inline">\(s_t\)</span> at time <span class="math inline">\(t\)</span>, and the state <span class="math inline">\(s_{t+1}\)</span> at time <span class="math inline">\(t+1\)</span>) are governed by the natural laws of the environment, and depend only on the most recent action <span class="math inline">\(a_t\)</span>. These can also be either deterministic</p>
<p><span class="math display">\[
s_{t+1} = f(s_t, a_t)
\]</span></p>
<p>or stochastic</p>
<p><span class="math display">\[
s_{t+1} \sim P(\cdot | s_t, a_t)
\]</span></p>
</section>
<section id="rewards-and-return" class="level2">
<h2 class="anchored" data-anchor-id="rewards-and-return">Rewards and Return</h2>
<p>The reward function <span class="math inline">\(R\)</span> is a crucial tool in RL. It depends on the current state of the world, the most recent action that was taken, and the next state of the world/environment:</p>
<p><span class="math display">\[
r_t = R(s_t, a_t, s_t+1)
\]</span></p>
<p>Although most of the time this is simplified to just a dependance on the current state, <span class="math inline">\(r_t = R(s_t)\)</span> or state action pair <span class="math inline">\(r_t = R(s_t, a_t)\)</span>. Moving forward, I will refer to the return of a trajectory as <span class="math inline">\(R(\tau)\)</span></p>
<p>There are 2 main kinds of return. The first is the <strong>finite-horizon undiscounted return</strong> which is just the sum of rewards obtained in a fixed window of steps</p>
<p><span class="math display">\[
R(\tau) = \sum_{t=0}^{T} r_t
\]</span></p>
<p>The second kind of return is the <strong>infinite-horizon undiscounted return</strong> which is the sum of all rewards ever obtained by the agent but discounted by how far in the future they are obtained. This forumalation ofthe reward function includes a discount factor <span class="math inline">\(\gamma \in (0,1)\)</span>:</p>
<p><span class="math display">\[
R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t
\]</span></p>
<p>Some people may think, why do we have the discount factor? do we not want to maximize all rewards? Intuitively, immediate rewards are more valuable that ones that are in the distant future, similar to the notion of money today is better than money later. Mathematically an infinite horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations and hence in code. But with a discount factor under resonable conditions, the infinite sum converges.</p>
</section>
<section id="the-rl-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-rl-problem">The RL Problem</h2>
<p>Whichever return measure we use (finite or infinite horizon) and whichever policy we choose, the goal in RL is to select a policy which maximized the <strong>expected return</strong> when the agent acts according to it. To talk about expectedd return, we first need to talk about probability distributions over trajectories.</p>
<p>Lets suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a <span class="math inline">\(T\)</span> step trajectory is:</p>
<p><span class="math display">\[
P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)\pi(a_t|s_t)
\]</span></p>
<p>The expected return (for whichever return function) denoted by <span class="math inline">\(J(\pi)\)</span> is then:</p>
<p><span class="math display">\[
J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \right]
\]</span></p>
<p>The main optimization problem in RL can then be expressed as</p>
<p><span class="math display">\[
\pi^{*} = argmax_\pi \ J(\pi)
\]</span></p>
<p>Where <span class="math inline">\(\pi^*\)</span> is the <strong>optimal policy</strong></p>
</section>
<section id="value-functions" class="level2">
<h2 class="anchored" data-anchor-id="value-functions">Value Functions</h2>
<p>Finally I will introduce the notion of value functions and then proceed to talk about a technique known as Q learning.</p>
<p>Oftentimes we want to know the <strong>value</strong> of a state, or state action pair. The value in this context means the expected return if you start in taht state or state action pair, and then act according to a particular policy.</p>
<p>There are 4 main functions of importance here</p>
<ol type="1">
<li>The <strong>On-Policy Value Function</strong>, <span class="math inline">\(V^\pi(s)\)</span>, which gives the expected return if you start in state <span class="math inline">\(s\)</span> and always act according to policy <span class="math inline">\(\pi\)</span>:</li>
</ol>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
\]</span></p>
<ol start="2" type="1">
<li>The <strong>On-Policy Action-Value Function</strong>, <span class="math inline">\(Q^\pi(s, a)\)</span>, which gives us the expected return if we start at state <span class="math inline">\(s\)</span> and take action <span class="math inline">\(a\)</span> then follow the policy <span class="math inline">\(\pi\)</span> indefinitely. The inital action <span class="math inline">\(a\)</span> does not necessarily need to come from the policy</li>
</ol>
<p><span class="math display">\[
Q^\pi(s, a) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
\]</span></p>
<ol start="3" type="1">
<li>The <strong>Optimal Value Function</strong>, <span class="math inline">\(V^*(s)\)</span> which gives the expected return if you start at state <span class="math inline">\(s\)</span> then always act according to the <em>optimal policy</em></li>
</ol>
<p><span class="math display">\[
V^*(s) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
\]</span></p>
<ol start="4" type="1">
<li>The <strong>Optimal Action Value Function</strong>, <span class="math inline">\(Q^*(s, a)\)</span> which gives the expected return if we start at state <span class="math inline">\(s\)</span> and take action <span class="math inline">\(a\)</span> then always act according to the <em>optimal policy</em></li>
</ol>
<p><span class="math display">\[
Q^*(s, a) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
\]</span></p>
</section>
<section id="bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="bellman-equations">Bellman Equations</h2>
<p>One of the most important concepts in reinforcement learning (RL) is the <strong>Bellman Equations</strong>. These equations form the mathematical backbone of RL by describing how the value of a state or action depends on the reward received and the values of subsequent states. In short the basic idea behind the bellman equations is this: *The value of being in a state is the reward you expect to get from being there, plus the value of where you end up next.</p>
<p>This concept is crucial because it allows us to break down the problem of finding the best actions over an entire sequence of decisions into smaller, easier to solve pieces.</p>
<p>The Bellman equations for the on-policy value functions are</p>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}_{a \sim \pi, s' \sim P} \left[ r(s, a) + \gamma V^\pi(s') \right]
\]</span> <span class="math display">\[
Q^\pi(s, a) = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ \mathbb{E}_{a' \sim \pi} \left[ Q^\pi(s', a') \right] \right]
\]</span></p>
<p>where <span class="math inline">\(s' \sim P\)</span> is shorthand for <span class="math inline">\(s' \sim P(\cdot | s, a)\)</span>, indicating that the next state <span class="math inline">\(s'\)</span> is sampled from the environment’s transition rules. Similarly, <span class="math inline">\(a \sim \pi\)</span> is shorthand for <span class="math inline">\(s \sim \pi(\cdot | s)\)</span> and <span class="math inline">\(a' \sim \pi\)</span> is shorthand for <span class="math inline">\(a' \sim \pi(\cdot | s')\)</span></p>
<p>The bellman equations for the optimal value functions are:</p>
<p><span class="math display">\[
V^*(s) = max_a \ \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma V^*(s') \right]
\]</span></p>
<p><span class="math display">\[
Q^* = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ max_{a'} \ Q^*(s',a') \right]
\]</span></p>
<section id="advantage-function" class="level3">
<h3 class="anchored" data-anchor-id="advantage-function">Advantage Function</h3>
<p>Sometimes in RL we need to describe how good an action is relative to the others and not in an absolute sense. This means that we want to know the relative advantage of that action. The advantage function <span class="math inline">\(A^\pi(s,a)\)</span> corresponsing to a policy <span class="math inline">\(\pi\)</span> describes how much better it is to take a specific action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, over randomly selecting an action according to <span class="math inline">\(\pi(\cdot|s)\)</span>. Mathematically the advantage function is defined as:</p>
<p><span class="math display">\[
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
\]</span></p>
</section>
</section>
<section id="policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="policy-optimization">Policy Optimization</h2>
<p>Policy optimization is a family of methods that try to explicitly represent a policy <span class="math inline">\(\pi_\theta(a|s)\)</span>. They optimize the parameters <span class="math inline">\(\theta\)</span> either directly by using gradient ascent on the performance objective <span class="math inline">\(J(\pi_\theta)\)</span>, or indirectly by maximizing local optimizations of <span class="math inline">\(J(\pi_\theta)\)</span>. This optimization is almost always performed <strong>on-policy</strong>, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator <span class="math inline">\(V_\phi(s)\)</span> for the on-policy value function <span class="math inline">\(V^\pi(s)\)</span>, which is used to figure out how to update the policy via its parameters.</p>
</section>
<section id="q-learning" class="level2">
<h2 class="anchored" data-anchor-id="q-learning">Q Learning</h2>
<p>Q-Learning is a family of methods that try to learn an approximator <span class="math inline">\(Q_\theta(s,a)\)</span> for the optimal action-value function <span class="math inline">\(Q*(s,a)\)</span>. Typically they use an objective function based on the Bellman Equations. This optimization is almost always performed <strong>off-policy</strong>, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. The corresponding policy is obtained via the connection between <span class="math inline">\(Q^*\)</span> and <span class="math inline">\(\pi^*\)</span>: the actions taken by the Q-Learning agent are given by</p>
<p><span class="math display">\[
a(s) = argmax_a \ Q_\theta(s,a)
\]</span></p>
</section>
</section>
<section id="solving-carracing-v3-using-proximal-policy-optimization-ppo" class="level1">
<h1>Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/FareesSiddiqui\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>