<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>An Introduction to Reinforcement Learning with Practical Examples â€“ index</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-2a7528f117d075273fca3a0b09f3bef2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-what-is-reinforcement-learning-rl" id="toc-introduction-what-is-reinforcement-learning-rl" class="nav-link active" data-scroll-target="#introduction-what-is-reinforcement-learning-rl">Introduction: What is Reinforcement Learning (RL)</a></li>
  <li><a href="#key-terminology-mathematical-background" id="toc-key-terminology-mathematical-background" class="nav-link" data-scroll-target="#key-terminology-mathematical-background">Key Terminology &amp; Mathematical Background</a>
  <ul>
  <li><a href="#policies" id="toc-policies" class="nav-link" data-scroll-target="#policies">Policies</a></li>
  <li><a href="#trajectories" id="toc-trajectories" class="nav-link" data-scroll-target="#trajectories">Trajectories</a></li>
  <li><a href="#rewards-and-return" id="toc-rewards-and-return" class="nav-link" data-scroll-target="#rewards-and-return">Rewards and Return</a></li>
  <li><a href="#the-rl-problem" id="toc-the-rl-problem" class="nav-link" data-scroll-target="#the-rl-problem">The RL Problem</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions">Value Functions</a></li>
  <li><a href="#bellman-equations" id="toc-bellman-equations" class="nav-link" data-scroll-target="#bellman-equations">Bellman Equations</a>
  <ul class="collapse">
  <li><a href="#advantage-function" id="toc-advantage-function" class="nav-link" data-scroll-target="#advantage-function">Advantage Function</a></li>
  </ul></li>
  <li><a href="#policy-optimization" id="toc-policy-optimization" class="nav-link" data-scroll-target="#policy-optimization">Policy Optimization</a></li>
  <li><a href="#q-learning" id="toc-q-learning" class="nav-link" data-scroll-target="#q-learning">Q Learning</a></li>
  </ul></li>
  <li><a href="#solving-carracing-v3-using-proximal-policy-optimization-ppo" id="toc-solving-carracing-v3-using-proximal-policy-optimization-ppo" class="nav-link" data-scroll-target="#solving-carracing-v3-using-proximal-policy-optimization-ppo">Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</a>
  <ul>
  <li><a href="#the-math-behind-ppo" id="toc-the-math-behind-ppo" class="nav-link" data-scroll-target="#the-math-behind-ppo">The Math Behind PPO</a></li>
  <li><a href="#the-carracing-problem" id="toc-the-carracing-problem" class="nav-link" data-scroll-target="#the-carracing-problem">The CarRacing problem</a></li>
  <li><a href="#implementing-the-solution-for-carracing" id="toc-implementing-the-solution-for-carracing" class="nav-link" data-scroll-target="#implementing-the-solution-for-carracing">Implementing the Solution for CarRacing</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">An Introduction to Reinforcement Learning with Practical Examples</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-what-is-reinforcement-learning-rl" class="level1">
<h1>Introduction: What is Reinforcement Learning (RL)</h1>
<p>I will start this article off with a little bit of mathematical background about Reinforcement Learning and how it works, so that readers have a strong base of knowledge and are able to understand some of the algorithms and methods at work.</p>
<p>Below is the layout of this article:</p>
<ul>
<li>What is Reinforcement Learning (RL)</li>
<li>Key Terminology
<ul>
<li>Policies</li>
<li>Trajectories</li>
<li>Rewards and Return</li>
<li>The formulation of an RL problem</li>
<li>Value Functions</li>
<li>Bellman Equations and the Advantage Function</li>
<li>Policy Optimization</li>
<li>Q Learning</li>
</ul></li>
<li>Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</li>
</ul>
<p>Reinforcement Learning (RL) is a machine learning paradigm inspired by how humans and animals learn through interactions with their environment. The core idea is that an <strong>agent</strong> learns to make decisions by interacting with an <strong>environment</strong>, aiming to maximize long term cumulative rewards.</p>
<p>The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees an observation of the state of the world (or environment), and then decides which action to take. The environment changes when the agent acts on it, but it may also change on its own.</p>
<p>The agent also perceives a <strong>reward</strong> from the environment. The reward is a number that tells the agent how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. This cumulative reward is known as the <strong>return</strong>. Reinforcement Learning methods are ways that the agent can learn behaviours to achieve its goal and maximize long term <em>return</em></p>
<p>Reinforcement Learning is built around the <strong>agent environment loop</strong>, as shown in the diagram below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images_quarto/agent_env.png" class="img-fluid figure-img"></p>
<figcaption>Agent Environment Paradigm</figcaption>
</figure>
</div>
</section>
<section id="key-terminology-mathematical-background" class="level1">
<h1>Key Terminology &amp; Mathematical Background</h1>
<p>Below I will explain how the agent environment interaction works, and the core terminology behind it as we will use it extensively later on in this article.</p>
<ul>
<li><p><strong>State <span class="math inline">\((S_t)\)</span></strong>: This is the current state of the environment. The environment provides the agent with a representation of its current situation, which we refer to as the state. For example, in a game, the state could be the positions of all characters on screen or in the scene.</p></li>
<li><p><strong>Action <span class="math inline">\((A_t)\)</span></strong>: Based on the current state, the agent chooses an action from its set of possible actions. The set of possible actions for an agent is known as the <strong>action space</strong>. In a simple self driving car, this could mean steering left, steering right, accelerate, or brake.</p></li>
</ul>
<section id="policies" class="level2">
<h2 class="anchored" data-anchor-id="policies">Policies</h2>
<p><strong>Policy <span class="math inline">\((\pi)\)</span></strong>: A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case itâ€™s usually denoted by <span class="math inline">\(\mu\)</span>:</p>
<p><span class="math display">\[
a_t = \mu(s_t)
\]</span></p>
<p>or it may be stochastic, in which case it is usually denoted by <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
a_t \sim \pi(\cdot | s_t)
\]</span></p>
<p><span class="math inline">\(a_t \sim \pi\)</span> means that <span class="math inline">\(a\)</span> is sampled from <span class="math inline">\(\pi\)</span>, since <span class="math inline">\(\pi\)</span> is a distribution.</p>
<p>In deep RL, we deal with <strong>parameterized policies</strong> which are policies whose outputs are computable functions that depend on a set of parameters (such as the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. Oftentimes the parameters of such a policy are denoted by <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\phi\)</span>, and are written as follow</p>
<p><span class="math display">\[
a_t = \mu_\theta(s_t) \ (deterministic \ policy)
\]</span> <span class="math display">\[
a_t \sim \pi_\theta(s_t) \ (stochastic \ policy)
\]</span></p>
</section>
<section id="trajectories" class="level2">
<h2 class="anchored" data-anchor-id="trajectories">Trajectories</h2>
<p>A trajectory <span class="math inline">\(\tau\)</span> is a sequence of states and actions in the world</p>
<p><span class="math display">\[
\tau = (s_0, a_0, s_1, a_1, \dots)
\]</span></p>
<p>The first state of the world, <span class="math inline">\(s_0\)</span> is randomly sampled from the <strong>start state distribution</strong>, which is sometimes denoted by <span class="math inline">\(\rho_0\)</span></p>
<p><span class="math display">\[
s_0 \sim \rho_0(\cdot)
\]</span></p>
<p>State transitions (what happens to the world between the state <span class="math inline">\(s_t\)</span> at time <span class="math inline">\(t\)</span>, and the state <span class="math inline">\(s_{t+1}\)</span> at time <span class="math inline">\(t+1\)</span>) are governed by the natural laws of the environment, and depend only on the most recent action <span class="math inline">\(a_t\)</span>. These can also be either deterministic</p>
<p><span class="math display">\[
s_{t+1} = f(s_t, a_t)
\]</span></p>
<p>or stochastic</p>
<p><span class="math display">\[
s_{t+1} \sim P(\cdot | s_t, a_t)
\]</span></p>
</section>
<section id="rewards-and-return" class="level2">
<h2 class="anchored" data-anchor-id="rewards-and-return">Rewards and Return</h2>
<p>The reward function <span class="math inline">\(R\)</span> is a crucial tool in RL. It depends on the current state of the world, the most recent action that was taken, and the next state of the world/environment:</p>
<p><span class="math display">\[
r_t = R(s_t, a_t, s_t+1)
\]</span></p>
<p>Although most of the time this is simplified to just a dependance on the current state, <span class="math inline">\(r_t = R(s_t)\)</span> or state action pair <span class="math inline">\(r_t = R(s_t, a_t)\)</span>. Moving forward, I will refer to the return of a trajectory as <span class="math inline">\(R(\tau)\)</span></p>
<p>There are 2 main kinds of return. The first is the <strong>finite-horizon undiscounted return</strong> which is just the sum of rewards obtained in a fixed window of steps</p>
<p><span class="math display">\[
R(\tau) = \sum_{t=0}^{T} r_t
\]</span></p>
<p>The second kind of return is the <strong>infinite-horizon undiscounted return</strong> which is the sum of all rewards ever obtained by the agent but discounted by how far in the future they are obtained. This forumalation ofthe reward function includes a discount factor <span class="math inline">\(\gamma \in (0,1)\)</span>:</p>
<p><span class="math display">\[
R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t
\]</span></p>
<p>Some people may think, why do we have the discount factor? do we not want to maximize all rewards? Intuitively, immediate rewards are more valuable that ones that are in the distant future, similar to the notion of money today is better than money later. Mathematically an infinite horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations and hence in code. But with a discount factor under resonable conditions, the infinite sum converges.</p>
</section>
<section id="the-rl-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-rl-problem">The RL Problem</h2>
<p>Whichever return measure we use (finite or infinite horizon) and whichever policy we choose, the goal in RL is to select a policy which maximized the <strong>expected return</strong> when the agent acts according to it. To talk about expectedd return, we first need to talk about probability distributions over trajectories.</p>
<p>Lets suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a <span class="math inline">\(T\)</span> step trajectory is:</p>
<p><span class="math display">\[
P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)\pi(a_t|s_t)
\]</span></p>
<p>The expected return (for whichever return function) denoted by <span class="math inline">\(J(\pi)\)</span> is then:</p>
<p><span class="math display">\[
J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \right]
\]</span></p>
<p>The main optimization problem in RL can then be expressed as</p>
<p><span class="math display">\[
\pi^{*} = argmax_\pi \ J(\pi)
\]</span></p>
<p>Where <span class="math inline">\(\pi^*\)</span> is the <strong>optimal policy</strong></p>
</section>
<section id="value-functions" class="level2">
<h2 class="anchored" data-anchor-id="value-functions">Value Functions</h2>
<p>Finally I will introduce the notion of value functions and then proceed to talk about a technique known as Q learning.</p>
<p>Oftentimes we want to know the <strong>value</strong> of a state, or state action pair. The value in this context means the expected return if you start in taht state or state action pair, and then act according to a particular policy.</p>
<p>There are 4 main functions of importance here</p>
<ol type="1">
<li>The <strong>On-Policy Value Function</strong>, <span class="math inline">\(V^\pi(s)\)</span>, which gives the expected return if you start in state <span class="math inline">\(s\)</span> and always act according to policy <span class="math inline">\(\pi\)</span>:</li>
</ol>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
\]</span></p>
<ol start="2" type="1">
<li>The <strong>On-Policy Action-Value Function</strong>, <span class="math inline">\(Q^\pi(s, a)\)</span>, which gives us the expected return if we start at state <span class="math inline">\(s\)</span> and take action <span class="math inline">\(a\)</span> then follow the policy <span class="math inline">\(\pi\)</span> indefinitely. The inital action <span class="math inline">\(a\)</span> does not necessarily need to come from the policy</li>
</ol>
<p><span class="math display">\[
Q^\pi(s, a) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
\]</span></p>
<ol start="3" type="1">
<li>The <strong>Optimal Value Function</strong>, <span class="math inline">\(V^*(s)\)</span> which gives the expected return if you start at state <span class="math inline">\(s\)</span> then always act according to the <em>optimal policy</em></li>
</ol>
<p><span class="math display">\[
V^*(s) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
\]</span></p>
<ol start="4" type="1">
<li>The <strong>Optimal Action Value Function</strong>, <span class="math inline">\(Q^*(s, a)\)</span> which gives the expected return if we start at state <span class="math inline">\(s\)</span> and take action <span class="math inline">\(a\)</span> then always act according to the <em>optimal policy</em></li>
</ol>
<p><span class="math display">\[
Q^*(s, a) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
\]</span></p>
</section>
<section id="bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="bellman-equations">Bellman Equations</h2>
<p>One of the most important concepts in reinforcement learning (RL) is the <strong>Bellman Equations</strong>. These equations form the mathematical backbone of RL by describing how the value of a state or action depends on the reward received and the values of subsequent states. In short the basic idea behind the bellman equations is this: *The value of being in a state is the reward you expect to get from being there, plus the value of where you end up next.</p>
<p>This concept is crucial because it allows us to break down the problem of finding the best actions over an entire sequence of decisions into smaller, easier to solve pieces.</p>
<p>The Bellman equations for the on-policy value functions are</p>
<p><span class="math display">\[
V^\pi(s) = \mathbb{E}_{a \sim \pi, s' \sim P} \left[ r(s, a) + \gamma V^\pi(s') \right]
\]</span> <span class="math display">\[
Q^\pi(s, a) = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ \mathbb{E}_{a' \sim \pi} \left[ Q^\pi(s', a') \right] \right]
\]</span></p>
<p>where <span class="math inline">\(s' \sim P\)</span> is shorthand for <span class="math inline">\(s' \sim P(\cdot | s, a)\)</span>, indicating that the next state <span class="math inline">\(s'\)</span> is sampled from the environmentâ€™s transition rules. Similarly, <span class="math inline">\(a \sim \pi\)</span> is shorthand for <span class="math inline">\(s \sim \pi(\cdot | s)\)</span> and <span class="math inline">\(a' \sim \pi\)</span> is shorthand for <span class="math inline">\(a' \sim \pi(\cdot | s')\)</span></p>
<p>The bellman equations for the optimal value functions are:</p>
<p><span class="math display">\[
V^*(s) = max_a \ \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma V^*(s') \right]
\]</span></p>
<p><span class="math display">\[
Q^* = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ max_{a'} \ Q^*(s',a') \right]
\]</span></p>
<section id="advantage-function" class="level3">
<h3 class="anchored" data-anchor-id="advantage-function">Advantage Function</h3>
<p>Sometimes in RL we need to describe how good an action is relative to the others and not in an absolute sense. This means that we want to know the relative advantage of that action. The advantage function <span class="math inline">\(A^\pi(s,a)\)</span> corresponsing to a policy <span class="math inline">\(\pi\)</span> describes how much better it is to take a specific action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>, over randomly selecting an action according to <span class="math inline">\(\pi(\cdot|s)\)</span>. Mathematically the advantage function is defined as:</p>
<p><span class="math display">\[
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
\]</span></p>
</section>
</section>
<section id="policy-optimization" class="level2">
<h2 class="anchored" data-anchor-id="policy-optimization">Policy Optimization</h2>
<p>Policy optimization is a family of methods that try to explicitly represent a policy <span class="math inline">\(\pi_\theta(a|s)\)</span>. They optimize the parameters <span class="math inline">\(\theta\)</span> either directly by using gradient ascent on the performance objective <span class="math inline">\(J(\pi_\theta)\)</span>, or indirectly by maximizing local optimizations of <span class="math inline">\(J(\pi_\theta)\)</span>. This optimization is almost always performed <strong>on-policy</strong>, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator <span class="math inline">\(V_\phi(s)\)</span> for the on-policy value function <span class="math inline">\(V^\pi(s)\)</span>, which is used to figure out how to update the policy via its parameters.</p>
</section>
<section id="q-learning" class="level2">
<h2 class="anchored" data-anchor-id="q-learning">Q Learning</h2>
<p>Q-Learning is a family of methods that try to learn an approximator <span class="math inline">\(Q_\theta(s,a)\)</span> for the optimal action-value function <span class="math inline">\(Q*(s,a)\)</span>. Typically they use an objective function based on the Bellman Equations. This optimization is almost always performed <strong>off-policy</strong>, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. The corresponding policy is obtained via the connection between <span class="math inline">\(Q^*\)</span> and <span class="math inline">\(\pi^*\)</span>: the actions taken by the Q-Learning agent are given by</p>
<p><span class="math display">\[
a(s) = argmax_a \ Q_\theta(s,a)
\]</span></p>
<hr>
</section>
</section>
<section id="solving-carracing-v3-using-proximal-policy-optimization-ppo" class="level1">
<h1>Solving CarRacing-v3 using Proximal Policy Optimization (PPO)</h1>
<p>What is PPO?</p>
<p>Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that belongs to the family of policy gradient methods, where the agent directly learns a parameterized policy <span class="math inline">\(\pi_\theta(a|s)\)</span> to maximize the cumulative reward in a given environment. The policy is usually parameterized via a nerual network.</p>
<p>PPO improves upon older policy gradient methods by addressing two key challenges:</p>
<ol type="1">
<li><p><strong>Instability in Policy Updates</strong>: Large updates to the policy can lead to suboptimal behavior or even collapse which just means that the agents policy becomes drastically worse or even completely non functional.</p></li>
<li><p><strong>Sample Inefficiency</strong>: Many algorithms waste training samples, making learning computationally expensive.</p></li>
</ol>
<p>PPO using a <strong>clipped objective function</strong> to limit how the policy can change in a single update, ensuring stability and efficiency. A clipped objective function ensures that the policy does not change too much in a single update. It limits the magnitude of changes in the probability ratio between the new and old policies. This will be discussed more rigorously a little deeper into this article.</p>
<section id="the-math-behind-ppo" class="level2">
<h2 class="anchored" data-anchor-id="the-math-behind-ppo">The Math Behind PPO</h2>
<p>PPO maximizes the <strong>expected total reward</strong>: <span class="math display">\[
J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^tr_t \right]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tau = (s_0, a_0, r_0, \dots, s_T, a_T, r_T)\)</span> is a trajectory of states, actions, and rewards.</li>
<li><span class="math inline">\(\gamma\)</span> is the discount factor. NOTE: <span class="math inline">\(\gamma \in (0, 1)\)</span></li>
<li><span class="math inline">\(r_t\)</span> is the reward at time <span class="math inline">\(t\)</span></li>
</ul>
<p>Instead of directly optimizing this, PPO introduces a surrogate objective to stabilize training:</p>
<ol type="1">
<li><strong>Probability Ratio</strong>:</li>
</ol>
<p><span class="math display">\[
r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{{\pi_\theta}_{old}(a_t|s_t)}
\]</span></p>
<p>This ratio compares the new policyâ€™s probability of choosing action <span class="math inline">\(a_t\)</span> with the old policyâ€™s probability</p>
<ol start="2" type="1">
<li><strong>Clipped Objective</strong>: PPO uses a clipped objective loss to constrain the policy update:</li>
</ol>
<p><span class="math display">\[
L^{CLIP}(\theta) = \mathbb{E} \left[min(r_t(\theta)A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t) \right ]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(A_t\)</span> is the <strong>advantage function</strong>, estimating how much better the action a_t was compared to the average</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant which controls the clipping range</li>
</ul>
<p>The clipping ensures that policy updates are small, preventing the agent from overfitting to noisy or overly optimistic advantage estimates.</p>
<ol start="3" type="1">
<li><strong>Value Function Loss</strong>: PPO also trains a value function V(s_t) to estimate the expected return from a state: <span class="math display">\[
L^{VF}(\theta) = \mathbb{E} \left[(V_\theta(s_t) - R_t)^2 \right ]
\]</span></li>
</ol>
<p>where <span class="math inline">\(R_t\)</span> is the discounted reward</p>
<ol start="4" type="1">
<li><span class="math inline">\(Final Loss\)</span>: The combined loss is defined mathematically as:</li>
</ol>
<p><span class="math display">\[
L(\theta) = L^{CLIP}(\theta) - c_1L^{VF}(\theta) + c_2\cdot Entropy \ Regularization
\]</span></p>
<ul>
<li>Entropy Regularization encourages exploration by penalizing overly deterministic policies</li>
<li><span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are weights</li>
</ul>
</section>
<section id="the-carracing-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-carracing-problem">The CarRacing problem</h2>
<p>CarRacing is a popular problem that is a part of the OpenAI Gymnasium for learning reinforcement learning. It is an environment which consists of a race car and randomized tracks, with the goal being to train an agent which can learn to drive the car optimally around all tracks.</p>
<p>To solve this problem we must first model it as a reinforcement learning problem, to do this we need to define it as a Markov Decision process (MDP). This involves specifying the states, actions, rewards, transition states, and the objective. Defining the MDP will guide us towards designing our RL solution. The documentation for the environment can be found <a href="https://gymnasium.farama.org/environments/box2d/car_racing/">here</a></p>
<hr>
<ol type="1">
<li><strong>State Space</strong> <span class="math inline">\((S)\)</span>: The state space in CarRacing-v3 (which is the environment we will be working with) consists of visual observations from a top down view of the racetrack</li>
</ol>
<ul>
<li><strong>Representation</strong>: Each state is an RGB image of size <span class="math inline">\(96 \times 96 \times 3\)</span>, showing the carâ€™s position relative to the racetrack.</li>
<li><strong>Example State</strong> : A snapshot of the track where the carâ€™s position, road layout, and off road areas are visible</li>
<li><strong>Challenges</strong>:
<ul>
<li>The state space is high dimensional (image based), requiring a convolutional neural network (CNN) for feature extraction</li>
<li>Temporal dependancies must be captured. For example, how the carâ€™s position changes over time</li>
</ul></li>
</ul>
<ol start="2" type="1">
<li><p><strong>Action Space</strong> <span class="math inline">\((A)\)</span>: The action space for this environment is continuous and consists of three control variables</p>
<ol type="1">
<li><strong>Steering</strong> (a_1):
<ul>
<li>Controls the carâ€™s direction</li>
<li>Has a range of <span class="math inline">\([-1, 1]\)</span> where <span class="math inline">\(-1\)</span> is a full left turn, <span class="math inline">\(1\)</span> is a full right turn, and <span class="math inline">\(0\)</span> means no steering</li>
</ul></li>
<li><strong>Acceleration</strong> <span class="math inline">\((s_2)\)</span>:
<ul>
<li>Controls the carâ€™s acceleration (gas pedal)</li>
<li>Has a range of <span class="math inline">\([0, 1]\)</span>, where <span class="math inline">\(0\)</span> means no acceleration and <span class="math inline">\(1\)</span> is maximum acceleration</li>
</ul></li>
<li><strong>Brake</strong> <span class="math inline">\((a_3)\)</span>:
<ul>
<li>Controls the carâ€™s brakes</li>
<li>Has a range of <span class="math inline">\([0, 1]\)</span> where <span class="math inline">\(0\)</span> means no brake pressure and <span class="math inline">\(1\)</span> means full brake preassure</li>
</ul></li>
</ol>
<ul>
<li><strong>Action Format</strong>: The action at each time step is a vector: <span class="math inline">\([a_1, a_2, a_3]\)</span></li>
<li><strong>Challenges</strong>
<ul>
<li>Continuous control demands precise adjustments of the action vector</li>
<li>Balancing steering, acceleration, and braking is non trivial, especially around sharp curves and tight corners (as we will see later)</li>
</ul></li>
</ul></li>
<li><p><strong>Reward Function</strong> <span class="math inline">\((R)\)</span>: The reward function incentivizes the agent to drive efficiently while also trying to stay on the track.</p>
<ul>
<li><strong>Positive Rewards</strong>:
<ul>
<li>The agent earns rewards proportional to its progress along the track.</li>
<li>Staying on the track while also progressing towards the finish lines yields the highest rewards.</li>
</ul></li>
<li><strong>Negative Rewards</strong>:
<ul>
<li>Penalties are applied the car goes off the road. The agent is also penalized with a reward of -0.1 every frame to encourage it to do something and explore the environment</li>
<li>Episodes terminate early if the car stays off of the road for too long, resulting in a large negative reward and causing the episode to end.</li>
</ul></li>
</ul></li>
<li><p><strong>Transition Dynamics</strong> <span class="math inline">\((P)\)</span>: The transitions are deterministic and governed by the physics engine of the environment</p>
<ul>
<li>Given a state <span class="math inline">\((s_t)\)</span> and action <span class="math inline">\((a_t)\)</span>,the environment computes the next state <span class="math inline">\((s_{t+1})\)</span> based on;
<ul>
<li>The carâ€™s velocity, orientation, and position</li>
<li>Physical forces acting on the car such as collisions</li>
</ul></li>
<li><strong>Modeling Assumptions</strong>: The agent must implicitly learn these dynamics through trial and error as they are not explicitly provided to the agent</li>
</ul></li>
<li><p><strong>Discount Factor</strong>: <span class="math inline">\((\gamma)\)</span></p></li>
</ol>
<p>The discount factor determines how much importance is placed on future rewards compared to immediate ones</p>
<ul>
<li><strong>Typical Value</strong>: <span class="math inline">\(\gamma = 0.99\)</span></li>
<li>This encourages the agent to focus on long term strategies such as planning ahead for sharp turns while still valuing immediate progress. This is just another hyperparameter that we can tune and the value for the discount factor ranges between <span class="math inline">\((0, 1)\)</span></li>
</ul>
<ol start="6" type="1">
<li><strong>Objective</strong> The agents objective is to learn a policy <span class="math inline">\(\pi_\theta(a|s)\)</span> that maximizes the <strong>expected total discounted return</strong> over an episode: <span class="math display">\[
J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^tr_t \right]
\]</span></li>
</ol>
<p>where:</p>
<ul>
<li><span class="math inline">\(\tau = (s_0, a_0, r_0, \dots, s_T, a_T, r_T)\)</span> is a trajectory of states, actions, and rewards.</li>
<li><span class="math inline">\(T\)</span> is the length of an episode</li>
</ul>
<p>This requires the agent to:</p>
<ol type="1">
<li>Learn to stay on track and complete laps efficiently</li>
<li>Maximize forward movement while minimizing penalties for off road driving or unnecessary braking</li>
</ol>
</section>
<section id="implementing-the-solution-for-carracing" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-solution-for-carracing">Implementing the Solution for CarRacing</h2>
<p>Now that we have covered the background, we can start to delve into the solution as well as designing and training an agent to solve the CarRacing-v3 environment.</p>
<p>First we will need to install the necessary libraries, all of them can be installed using the following commnd</p>
<pre><code>pip install gymnasium gymnasium[box2d] stable-baselines3</code></pre>
<p>we will be using the gymnasium library developed by OpenAI as it will provide is with our environment. we can load and test that everything is working with the following code:</p>
<div id="1f6c5f3e" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"CarRacing-v3"</span>, continuous<span class="op">=</span><span class="va">True</span>, render_mode<span class="op">=</span><span class="st">"human"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>state, info <span class="op">=</span> env.reset()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> time.time()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> env.action_space.sample()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    next_state, reward, done, truncated, info <span class="op">=</span> env.step(action)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> time.time() <span class="op">-</span> start <span class="op">&gt;</span> <span class="dv">1000</span>:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>env.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above code should launch a pygame window containing the environment. Below is a screenshot of what this should look like:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images_quarto/env_startup.gif" class="img-fluid figure-img"></p>
<figcaption>CarRacing-v3</figcaption>
</figure>
</div>
<p>Currently there is no agent so the car is just choosing random actions from itâ€™s action space. The next step is to train a parameterized policy via a neural network. To do this we will use stable_baselines3, this is a library that implements the frame for a lot of deep reinforcement learning algorithms such as PPO.</p>
<p>Make sure we include all the necesssary libraries.</p>
<div id="0959c9b2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3 <span class="im">import</span> PPO</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.vec_env <span class="im">import</span> VecTransposeImage, DummyVecEnv, SubprocVecEnv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then need to define our environments, to make sure we are efficiently exploring the environment we can run multiple environments in parallel and have the policy use all of the trajectories from each environment in our calculations. The more environments we add the better the agent should perform, but we have to be careful not to allocate too many as it could overload the cpu.</p>
<div id="73f760d4" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_env():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init():</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> gym.make(<span class="st">"CarRacing-v3"</span>, continuous<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _init</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>num_envs <span class="op">=</span> <span class="dv">4</span>  <span class="co"># This can be tuned based on your cpu</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> SubprocVecEnv([make_env() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_envs)])</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> VecTransposeImage(env)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the above cell, <code>SubProcVecEnv()</code> is used to parallelize the environments. This allows the agent to collect experiences (states, actions, rewards) from multiple environments simultaneously.</p>
<p><code>VecTransposeImage()</code> is used to transpose the image fata to ensure that the RGB channels are last in the shape of the matrix. This is because the stable_baselines library expects the data to be in this format, so this function is just being used to ensure that we follow that format.</p>
<p>Next we can try to learn the policy using PPO.</p>
<div id="60aada81" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>total_timesteps <span class="op">=</span> <span class="dv">1_000_000</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PPO(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    policy<span class="op">=</span><span class="st">"CnnPolicy"</span>,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    env<span class="op">=</span>env,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">"cuda"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">3e-4</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    n_steps<span class="op">=</span>n_steps,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    n_epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    gamma<span class="op">=</span><span class="fl">0.99</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    gae_lambda<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    clip_range<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    ent_coef<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>model.learn(total_timesteps<span class="op">=</span>total_timesteps)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">"ppo_carracing_gpu_4envs_attempt2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above code is pretty self explanatory, and since we covered the mathematical background for PPO, we should have a good understanding of what each of these paramters are doing. <code>model.learn(total_timesteps)</code> just invokes the training loop for the agent, and <code>model.save()</code> just saves the weights to disk.</p>
<p>Using this policy we should get a much better agent that is able to drive much better than the initial agent which was choosing random actions from its action space. Below are my results for the agent.</p>
<p>ADD GIF OF TRAINED AGENT AFTER TRAINING IS DONE</p>
<!-- Thanks to stable_baselines3, training the agent is actually quite simple. Although we did not need the mathematical background, understanding it will give us a much better understanding of what is going on and what each of these parameters mean. As a refresher, here is an explanation of each of the parameters: -->
<!-- should I do this? -->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/FareesSiddiqui\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>