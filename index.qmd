---
title: "An Introduction to Reinforcement Learning with Practical Examples"
execute: 
  echo: true
  eval: false
format:
  html:
    code-fold: false
    theme: darkly
    toc: true
    toc-depth: 4
    toc-expand: true
project:
  type: website
  output-dir: docs
jupyter: python3
---

# Introduction: What is Reinforcement Learning (RL)
I will start this article off with a little bit of mathematical background about Reinforcement Learning and how it works, so that readers have a strong base of knowledge and are able to understand some of the algorithms and methods at work.

Below is the layout of this article:

- What is Reinforcement Learning (RL)
- Key Terminology
  - Policies
  - Trajectories
  - Rewards and Return
  - The formulation of an RL problem
  - Value Functions
  - Bellman Equations and the Advantage Function
  - Policy Optimization
  - Q Learning
- Solving CarRacing-v3 using Proximal Policy Optimization (PPO)


Reinforcement Learning (RL) is a machine learning paradigm inspired by how humans and animals learn through interactions with their environment. The core idea is that an **agent** learns to make decisions by interacting with an **environment**, aiming to maximize long term cumulative rewards. 

The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees an observation of the state of the world (or environment), and then decides which action to take. The environment changes when the agent acts on it, but it may also change on its own.

The agent also perceives a **reward** from the environment. The reward is a number that tells the agent how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward. This cumulative reward is known as the **return**. Reinforcement Learning methods are ways that the agent can learn behaviours to achieve its goal and maximize long term *return*

Reinforcement Learning is built around the **agent environment loop**, as shown in the diagram below:

![Agent Environment Paradigm](images_quarto/agent_env.png)

# Key Terminology & Mathematical Background

Below I will explain how the agent environment interaction works, and the core terminology behind it as we will use it extensively later on in this article.

- **State $(S_t)$**: This is the current state of the environment. The environment provides the agent with a representation of its current situation, which we refer to as the state. For example, in a game, the state could be the positions of all characters on screen or in the scene.

- **Action $(A_t)$**: Based on the current state, the agent chooses an action from its set of possible actions. The set of possible actions for an agent is known as the **action space**. In a simple self driving car, this could mean steering left, steering right, accelerate, or brake.

## Policies

**Policy $(\pi)$**: A policy is a rule used by an agent to decide what actions to take. It can be deterministic, in which case it's usually denoted by $\mu$:

$$
a_t = \mu(s_t)
$$

or it may be stochastic, in which case it is usually denoted by $\pi$:

$$
a_t \sim \pi(\cdot | s_t)
$$

$a_t \sim \pi$ means that $a$ is sampled from $\pi$, since $\pi$ is a distribution. 

In deep RL, we deal with **parameterized policies** which are policies whose outputs are computable functions that depend on a set of parameters (such as the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm. Oftentimes the parameters of such a policy are denoted by $\theta$ or $\phi$, and are written as follow

$$
a_t = \mu_\theta(s_t) \ (deterministic \ policy)
$$
$$
a_t \sim \pi_\theta(s_t) \ (stochastic \ policy)
$$

## Trajectories

A trajectory $\tau$ is a sequence of states and actions in the world

$$
\tau = (s_0, a_0, s_1, a_1, \dots)
$$

The first state of the world, $s_0$ is randomly sampled from the **start state distribution**, which is sometimes denoted by $\rho_0$

$$
s_0 \sim \rho_0(\cdot)
$$

State transitions (what happens to the world between the state $s_t$ at time $t$, and the state $s_{t+1}$ at time $t+1$) are governed by the natural laws of the environment, and depend only on the most recent action $a_t$. These can also be either deterministic

$$
s_{t+1} = f(s_t, a_t)
$$

or stochastic

$$
s_{t+1} \sim P(\cdot | s_t, a_t)
$$

## Rewards and Return

The reward function $R$ is a crucial tool in RL. It depends on the current state of the world, the most recent action that was taken, and the next state of the world/environment:

$$
r_t = R(s_t, a_t, s_t+1)
$$

Although most of the time this is simplified to just a dependance on the current state, $r_t = R(s_t)$ or state action pair $r_t = R(s_t, a_t)$. Moving forward, I will refer to the return of a trajectory as $R(\tau)$

There are 2 main kinds of return. The first is the **finite-horizon undiscounted return** which is just the sum of rewards obtained in a fixed window of steps

$$
R(\tau) = \sum_{t=0}^{T} r_t
$$

The second kind of return is the **infinite-horizon undiscounted return** which is the sum of all rewards ever obtained by the agent but discounted by how far in the future they are obtained. This forumalation ofthe reward function includes a discount factor $\gamma \in (0,1)$:

$$
R(\tau) = \sum_{t=0}^{\infty} \gamma^t r_t
$$

Some people may think, why do we have the discount factor? do we not want to maximize all rewards? Intuitively, immediate rewards are more valuable that ones that are in the distant future, similar to the notion of money today is better than money later. Mathematically an infinite horizon sum of rewards may not converge to a finite value, and is hard to deal with in equations and hence in code. But with a discount factor under resonable conditions, the infinite sum converges.

## The RL Problem

Whichever return measure we use (finite or infinite horizon) and whichever policy we choose, the goal in RL is to select a policy which maximized the **expected return** when the agent acts according to it. To talk about expectedd return, we first need to talk about probability distributions over trajectories. 

Lets suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a $T$ step trajectory is:

$$
P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1} P(s_{t+1}|s_t, a_t)\pi(a_t|s_t)
$$

The expected return (for whichever return function) denoted by $J(\pi)$ is then:

$$
J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) \right]
$$

The main optimization problem in RL can then be expressed as 

$$
\pi^{*} = argmax_\pi \ J(\pi)
$$

Where $\pi^*$ is the **optimal policy**

## Value Functions

Finally I will introduce the notion of value functions and then proceed to talk about a technique known as Q learning.

Oftentimes we want to know the **value** of a state, or state action pair. The value in this context means the expected return if you start in taht state or state action pair, and then act according to a particular policy.

There are 4 main functions of importance here

1. The **On-Policy Value Function**, $V^\pi(s)$, which gives the expected return if you start in state $s$ and always act according to policy $\pi$:

$$
V^\pi(s) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
$$

2. The **On-Policy Action-Value Function**, $Q^\pi(s, a)$, which gives us the expected return if we start at state $s$ and take action $a$ then follow the policy $\pi$ indefinitely. The inital action $a$ does not necessarily need to come from the policy

$$
Q^\pi(s, a) = \mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
$$

3. The **Optimal Value Function**, $V^*(s)$ which gives the expected return if you start at state $s$ then always act according to the *optimal policy*

$$
V^*(s) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s\right]
$$

4. The **Optimal Action Value Function**, $Q^*(s, a)$ which gives the expected return if we start at state $s$ and take action $a$ then always act according to the *optimal policy*

$$
Q^*(s, a) = max_\pi\mathbb{E}_{\tau \sim \pi} \left[ R(\tau) | s_0 = s, a_0 = a\right]
$$

## Bellman Equations
One of the most important concepts in reinforcement learning (RL) is the **Bellman Equations**. These equations form the mathematical backbone of RL by describing how the value of a state or action depends on the reward received and the values of subsequent states. In short the basic idea behind the bellman equations is this: *The value of being in a state is the reward you expect to get from being there, plus the value of where you end up next.

This concept is crucial because it allows us to break down the problem of finding the best actions over an entire sequence of decisions into smaller, easier to solve pieces.

The Bellman equations for the on-policy value functions are

$$
V^\pi(s) = \mathbb{E}_{a \sim \pi, s' \sim P} \left[ r(s, a) + \gamma V^\pi(s') \right]
$$
$$
Q^\pi(s, a) = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ \mathbb{E}_{a' \sim \pi} \left[ Q^\pi(s', a') \right] \right]
$$

where $s' \sim P$ is shorthand for $s' \sim P(\cdot | s, a)$, indicating that the next state $s'$ is sampled from the environment's transition rules. Similarly, $a \sim \pi$ is shorthand for $s \sim \pi(\cdot | s)$ and $a' \sim \pi$ is shorthand for $a' \sim \pi(\cdot | s')$

The bellman equations for the optimal value functions are:

$$
V^*(s) = max_a \ \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma V^*(s') \right]
$$

$$
Q^* = \mathbb{E}_{s' \sim P} \left[ r(s, a) + \gamma \ max_{a'} \ Q^*(s',a') \right]
$$

### Advantage Function

Sometimes in RL we need to describe how good an action is relative to the others and not in an absolute sense. This means that we want to know the relative advantage of that action. The advantage function $A^\pi(s,a)$ corresponsing to a policy $\pi$ describes how much better it is to take a specific action $a$ in state $s$, over randomly selecting an action according to $\pi(\cdot|s)$. Mathematically the advantage function is defined as:

$$
A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)
$$

## Policy Optimization
Policy optimization is a family of methods that try to explicitly represent a policy $\pi_\theta(a|s)$. They optimize the parameters $\theta$ either directly by using gradient ascent on the performance objective $J(\pi_\theta)$, or indirectly by maximizing local optimizations of $J(\pi_\theta)$. This optimization is almost always performed **on-policy**, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator $V_\phi(s)$ for the on-policy value function $V^\pi(s)$, which is used to figure out how to update the policy via its parameters.

## Q Learning
Q-Learning is a family of methods that try to learn an approximator $Q_\theta(s,a)$ for the optimal action-value function $Q*(s,a)$. Typically they use an objective function based on the Bellman Equations. This optimization is almost always performed **off-policy**, which means that each update can use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was obtained. The corresponding policy is obtained via the connection between $Q^*$ and $\pi^*$: the actions taken by the Q-Learning agent are given by

$$
a(s) = argmax_a \ Q_\theta(s,a)
$$

# Solving CarRacing-v3 using Proximal Policy Optimization (PPO)